{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9SUqHD5P9MH"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install datasets\n",
        "from transformers import BertForQuestionAnswering, BertTokenizer, BertConfig\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AdamW\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased').to('cuda')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Fine-tuning settings\n",
        "learning_rate = 5e-5\n",
        "batch_size = 8\n",
        "num_epochs = 3\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Fine-tune the model on SQuAD dataset\n",
        "# You should replace this with your own SQuAD dataset\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('squad')\n",
        "\n",
        "train_dataset = dataset['train']\n",
        "eval_dataset = dataset['validation']\n",
        "\n",
        "# Tokenize the questions, contexts, and answers\n",
        "tokenized_inputs = tokenizer(train_dataset['question'], train_dataset['context'], padding=True, truncation=True, return_tensors='pt'\n",
        "\n",
        "# Create start and end positions\n",
        "start_positions = train_dataset['start_positions']\n",
        "end_positions = train_dataset['end_positions']\n",
        "\n",
        "start_positions_tensor = torch.tensor(start_positions)\n",
        "end_positions_tensor = torch.tensor(end_positions)\n",
        "\n",
        "# Create a TensorDataset\n",
        "dataset = TensorDataset(tokenized_inputs['input_ids'], tokenized_inputs['attention_mask'], start_positions_tensor, end_positions_tensor)\n",
        "\n",
        "# Create a DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Fine-tune the model\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in tqdm(dataloader, desc=\"Epoch {}\".format(epoch + 1)):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, start_positions, end_positions = batch\n",
        "        outputs = model(input_ids.to('cuda'), attention_mask=attention_mask.to('cuda'), start_positions=start_positions.to('cuda'), end_positions=end_positions.to('cuda'))\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained('fine_tuned_model')\n",
        "\n",
        "# Inference using the fine-tuned model\n",
        "question = \"Who wrote Romeo and Juliet?\"\n",
        "context = \"Romeo and Juliet was written by William Shakespeare.\"\n",
        "inputs = tokenizer(question, context, return_tensors='pt')\n",
        "inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
        "start_logits, end_logits = model(**inputs).values()\n",
        "\n",
        "answer_start = torch.argmax(start_logits)\n",
        "answer_end = torch.argmax(end_logits) + 1\n",
        "answer_tokens = inputs['input_ids'][0][answer_start:answer_end].tolist()\n",
        "\n",
        "answer = tokenizer.decode(answer_tokens)\n",
        "\n",
        "print(\"Question:\", question)\n",
        "print(\"Predicted Answer:\", answer)\n"
      ]
    }
  ]
}